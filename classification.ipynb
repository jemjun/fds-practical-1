{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6de3f618",
   "metadata": {},
   "source": [
    "# Classification Problems in Machine Learning\n",
    "\n",
    "## What is Classification?\n",
    "\n",
    "Classification is a supervised learning task where the goal is to predict discrete categories or classes for input data. Unlike regression (which predicts continuous values), classification assigns each input to one of a predefined set of classes.\n",
    "\n",
    "## Types of Classification\n",
    "\n",
    "**Binary Classification**: Two possible classes (e.g., spam/not spam, positive/negative)\n",
    "\n",
    "**Multi-class Classification**: More than two mutually exclusive classes (e.g., cat/dog/bird) <- **This is what Practical 1 is about!**\n",
    "\n",
    "**Multi-label Classification**: Multiple non-exclusive labels can apply simultaneously (e.g., a movie can be both \"action\" and \"comedy\")\n",
    "\n",
    "## Example: Email Spam Detection\n",
    "\n",
    "Consider building a spam filter for emails. Each email needs to be classified as either \"spam\" or \"not spam\" based on features (matrix $X$) like:\n",
    "- Presence of certain keywords (\"free\", \"winner\", \"click here\")\n",
    "- Sender reputation\n",
    "- Email length\n",
    "- Number of links\n",
    "- Semantic embeddings\n",
    "\n",
    "Given a training dataset of labeled emails, a classification model learns patterns that distinguish spam from legitimate emails, then predicts the class for new, unseen emails.\n",
    "\n",
    "\n",
    "For instance, the feature matrix $X$ for 3 emails might look like:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "\\text{keyword\\_count} & \\text{sender\\_rep} & \\text{length} & \\text{links} \\\\\n",
    "5 & 0.2 & 150 & 8  \\\\\n",
    "0 & 0.9 & 450 & 1  \\\\\n",
    "2 & 0.1 & 75 & 12 \n",
    "\\end{bmatrix}, \\quad y = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where each row represents an email, columns represent features [keyword count, sender reputation, length, # links], and $y$ indicates whether each email is spam (1) or not spam (0).\n",
    "\n",
    "## Key Metrics for Classification\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "A confusion matrix shows the breakdown of predictions:\n",
    "\n",
    "|                | Predicted Positive | Predicted Negative |\n",
    "|----------------|-------------------|-------------------|\n",
    "| **Actually Positive** | True Positive (TP) | False Negative (FN) |\n",
    "| **Actually Negative** | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "### Precision\n",
    "\n",
    "Precision measures the accuracy of positive predictions:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "**Interpretation**: \"Of all emails we flagged as spam, how many were actually spam?\"\n",
    "\n",
    "### Recall (Sensitivity)\n",
    "\n",
    "Recall measures how many actual positives we found:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "**Interpretation**: \"Of all actual spam emails, how many did we catch?\"\n",
    "\n",
    "### F1 Score\n",
    "\n",
    "F1 score is the harmonic mean of precision and recall, providing a single balanced metric:\n",
    "\n",
    "$$\n",
    "F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "**Why harmonic mean?** It penalizes extreme values. A model with 100% recall but 10% precision would have a low F1 score, ensuring balance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a71e81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[6 1]\n",
      " [2 6]]\n",
      "\n",
      "Breakdown:\n",
      "True Negatives (TN):  6\n",
      "False Positives (FP): 1\n",
      "False Negatives (FN): 2\n",
      "True Positives (TP):  6\n",
      "\n",
      "Precision: 0.857\n",
      "Recall:    0.750\n",
      "F1 Score:  0.800\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not Spam       0.75      0.86      0.80         7\n",
      "        Spam       0.86      0.75      0.80         8\n",
      "\n",
      "    accuracy                           0.80        15\n",
      "   macro avg       0.80      0.80      0.80        15\n",
      "weighted avg       0.81      0.80      0.80        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Example: Email spam classification results\n",
    "# 1 = spam, 0 = not spam\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0])\n",
    "y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0])\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nBreakdown:\")\n",
    "print(f\"True Negatives (TN):  {cm[0, 0]}\")\n",
    "print(f\"False Positives (FP): {cm[0, 1]}\")\n",
    "print(f\"False Negatives (FN): {cm[1, 0]}\")\n",
    "print(f\"True Positives (TP):  {cm[1, 1]}\")\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"\\nPrecision: {precision:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")\n",
    "print(f\"F1 Score:  {f1:.3f}\")\n",
    "\n",
    "# Comprehensive report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Not Spam', 'Spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ec5015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Precision: 0.857\n",
      "Manual Recall:    0.750\n",
      "Manual F1:        0.800\n"
     ]
    }
   ],
   "source": [
    "# Manual calculation to understand the formulas\n",
    "TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "\n",
    "precision_manual = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall_manual = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual) if (precision_manual + recall_manual) > 0 else 0\n",
    "\n",
    "print(f\"Manual Precision: {precision_manual:.3f}\")\n",
    "print(f\"Manual Recall:    {recall_manual:.3f}\")\n",
    "print(f\"Manual F1:        {f1_manual:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7776b69",
   "metadata": {},
   "source": [
    "## When to Use Which Metric?\n",
    "\n",
    "**Precision is critical when**: False positives are costly\n",
    "- Example: Medical diagnosis - you don't want to tell healthy patients they're sick\n",
    "\n",
    "**Recall is critical when**: False negatives are costly\n",
    "- Example: Fraud detection - you don't want to miss actual fraud cases\n",
    "\n",
    "**F1 Score is useful when**: You need a balance between precision and recall, especially with imbalanced datasets\n",
    "\n",
    "## Trade-offs\n",
    "\n",
    "There's often a trade-off between precision and recall. By adjusting the classification threshold, you can:\n",
    "- **Increase precision** → Lower recall (stricter about positive predictions)\n",
    "- **Increase recall** → Lower precision (more lenient about positive predictions)\n",
    "\n",
    "The F1 score helps you find a good balance between these two metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647dc67b",
   "metadata": {},
   "source": [
    "# Confidence-Weighted F1 Score \n",
    "\n",
    "For a classifier that outputs predicted labels $\\hat{y}_i$ and confidence scores $c_i \\in [0,1]$ for each sample $i$, the symmetric confidence-weighted precision and recall are:\n",
    "\n",
    "$\n",
    "\\text{Precision}_{\\text{weighted}} = \\frac{\\sum_{i=1}^{n} c_i \\cdot \\mathbb{1}(\\hat{y}_i = 1 \\land y_i = 1)}{\\sum_{i=1}^{n} c_i \\cdot \\mathbb{1}(\\hat{y}_i = 1 \\land y_i = 1) + \\sum_{i=1}^{n} c_i \\cdot \\mathbb{1}(\\hat{y}_i = 1 \\land y_i = 0)}\n",
    "$\n",
    "\n",
    "$\n",
    "\\text{Recall}_{\\text{weighted}} = \\frac{\\sum_{i=1}^{n} c_i \\cdot \\mathbb{1}(\\hat{y}_i = 1 \\land y_i = 1)}{\\sum_{i=1}^{n} c_i \\cdot \\mathbb{1}(\\hat{y}_i = 1 \\land y_i = 1) + \\sum_{i=1}^{n} c_i \\cdot \\mathbb{1}(\\hat{y}_i = 0 \\land y_i = 1)}\n",
    "$\n",
    "\n",
    "Or more concisely:\n",
    "\n",
    "$\n",
    "\\text{Precision}_{\\text{weighted}} = \\frac{\\text{Weighted TP}}{\\text{Weighted TP} + \\text{Weighted FP}}\n",
    "$\n",
    "\n",
    "$\n",
    "\\text{Recall}_{\\text{weighted}} = \\frac{\\text{Weighted TP}}{\\text{Weighted TP} + \\text{Weighted FN}}\n",
    "$\n",
    "\n",
    "The confidence-weighted F1 score is then:\n",
    "\n",
    "$\n",
    "F1_{\\text{weighted}} = 2 \\cdot \\frac{\\text{Precision}_{\\text{weighted}} \\cdot \\text{Recall}_{\\text{weighted}}}{\\text{Precision}_{\\text{weighted}} + \\text{Recall}_{\\text{weighted}}}\n",
    "$\n",
    "\n",
    "**Where:**\n",
    "- $n$ is the number of samples\n",
    "- $y_i$ is the true label for sample $i$\n",
    "- $\\hat{y}_i$ is the predicted label for sample $i$\n",
    "- $c_i$ is the confidence score for sample $i$\n",
    "- $\\mathbb{1}(\\cdot)$ is the indicator function\n",
    "- Weighted TP = $\\sum_{i=1}^{n} c_i \\cdot \\mathbb{1}(\\hat{y}_i = 1 \\land y_i = 1)$\n",
    "- Weighted FP = $\\sum_{i=1}^{n} c_i \\cdot \\mathbb{1}(\\hat{y}_i = 1 \\land y_i = 0)$\n",
    "- Weighted FN = $\\sum_{i=1}^{n} c_i \\cdot \\mathbb{1}(\\hat{y}_i = 0 \\land y_i = 1)$\n",
    "\n",
    "**Key Properties:**\n",
    "- All predictions (both positive and negative) are weighted by their confidence scores\n",
    "- High-confidence errors are penalized more heavily than low-confidence errors\n",
    "- The metric rewards models that are both accurate and well-calibrated\n",
    "- Unlike the asymmetric version, false negatives are also weighted by the confidence of the negative prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15a60277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_weighted_f1(y_true, y_pred, confidence):\n",
    "    # Weight all predictions, not just positives\n",
    "    weighted_tp = np.sum(confidence * (y_pred == 1) * (y_true == 1))\n",
    "    weighted_fp = np.sum(confidence * (y_pred == 1) * (y_true == 0))\n",
    "    weighted_fn = np.sum(confidence * (y_pred == 0) * (y_true == 1))\n",
    "    \n",
    "    precision = weighted_tp / (weighted_tp + weighted_fp) if (weighted_tp + weighted_fp) > 0 else 0\n",
    "    recall = weighted_tp / (weighted_tp + weighted_fn) if (weighted_tp + weighted_fn) > 0 else 0\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29c78460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True labels:      [1 0 1 1 0 1 0 1]\n",
      "Predicted labels: [1 0 1 0 0 1 1 1]\n",
      "Confidence:       [1 1 1 1 1 1 1 1]\n",
      "\n",
      "Confidence-weighted F1 score: 0.8000\n",
      "Standard F1 score:            0.8000\n"
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 1])\n",
    "y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 1])\n",
    "\n",
    "#confidence = np.array([0.9, 0.8, 0.95, 0.4, 0.9, 0.85, 0.25, 0.95])\n",
    "confidence = np.array([1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "f1_weighted = confidence_weighted_f1(y_true, y_pred, confidence)\n",
    "\n",
    "print(\"True labels:     \", y_true)\n",
    "print(\"Predicted labels:\", y_pred)\n",
    "print(\"Confidence:      \", confidence)\n",
    "print(f\"\\nConfidence-weighted F1 score: {f1_weighted:.4f}\")\n",
    "\n",
    "# Compare with standard F1 (all confidences = 1)\n",
    "from sklearn.metrics import f1_score\n",
    "f1_standard = f1_score(y_true, y_pred)\n",
    "print(f\"Standard F1 score:            {f1_standard:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed5a3acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True labels:      [1 0 1 1 0 1 0 1]\n",
      "Predicted labels: [1 0 1 0 0 1 1 1]\n",
      "Confidence:       [1.  1.  1.  0.5 1.  1.  1.  1. ]\n",
      "\n",
      "Confidence-weighted F1 score: 0.8421\n",
      "Standard F1 score:            0.8000\n"
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 1])\n",
    "y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 1])\n",
    "confidence = np.array([1, 1, 1, 0.5, 1, 1, 1, 1])\n",
    "\n",
    "f1_weighted = confidence_weighted_f1(y_true, y_pred, confidence)\n",
    "\n",
    "print(\"True labels:     \", y_true)\n",
    "print(\"Predicted labels:\", y_pred)\n",
    "print(\"Confidence:      \", confidence)\n",
    "print(f\"\\nConfidence-weighted F1 score: {f1_weighted:.4f}\")\n",
    "\n",
    "# Compare with standard F1 (all confidences = 1)\n",
    "from sklearn.metrics import f1_score\n",
    "f1_standard = f1_score(y_true, y_pred)\n",
    "print(f\"Standard F1 score:            {f1_standard:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe1e3db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True labels:      [1 0 1 1 0 1 0 1]\n",
      "Predicted labels: [1 0 1 0 0 1 1 1]\n",
      "Confidence:       [0.9 1.  1.  0.8 1.  1.  0.8 1. ]\n",
      "\n",
      "Confidence-weighted F1 score: 0.8298\n",
      "Standard F1 score:            0.8000\n"
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 1])\n",
    "y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 1])\n",
    "confidence = np.array([0.9, 1, 1, 0.8, 1, 1, 0.8, 1])\n",
    "\n",
    "f1_weighted = confidence_weighted_f1(y_true, y_pred, confidence)\n",
    "\n",
    "print(\"True labels:     \", y_true)\n",
    "print(\"Predicted labels:\", y_pred)\n",
    "print(\"Confidence:      \", confidence)\n",
    "print(f\"\\nConfidence-weighted F1 score: {f1_weighted:.4f}\")\n",
    "\n",
    "# Compare with standard F1 (all confidences = 1)\n",
    "from sklearn.metrics import f1_score\n",
    "f1_standard = f1_score(y_true, y_pred)\n",
    "print(f\"Standard F1 score:            {f1_standard:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447436d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
